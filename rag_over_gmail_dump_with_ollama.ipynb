{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef9c6c-2394-4a82-a2a5-41d8083184cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community chromadb ollama\n",
    "\n",
    "# Need to install Ollama from their website (https://ollama.com/download) and then run\n",
    "# ollama pull nomic-embed-text\n",
    "# ollama pull llama2\n",
    "# ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d31303-b15e-4acc-b32e-71a4394f0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "\n",
      "Creating new vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/wgqdp8h93bxg63tf8ywyg4n800t0c0/T/ipykernel_31249/3908575990.py:27: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  self.embeddings = OllamaEmbeddings(model=embedding_model)\n",
      "/var/folders/sc/wgqdp8h93bxg63tf8ywyg4n800t0c0/T/ipykernel_31249/3908575990.py:144: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening mbox file: /Users/jigneshjain/Documents/top_100_emails.mbox\n",
      "Found 100 emails in mbox file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing emails: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 507.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 with 100 emails\n",
      "Created 1761 chunks from this batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/wgqdp8h93bxg63tf8ywyg4n800t0c0/T/ipykernel_31249/3908575990.py:170: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  self.vectorstore.persist()\n",
      "/var/folders/sc/wgqdp8h93bxg63tf8ywyg4n800t0c0/T/ipykernel_31249/3908575990.py:185: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=model, temperature=0.1)\n",
      "/var/folders/sc/wgqdp8h93bxg63tf8ywyg4n800t0c0/T/ipykernel_31249/3908575990.py:226: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.rag_chain({\"query\": query, \"question\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks in vector store: 1761\n",
      "Query processed in 9.64 seconds\n",
      "\n",
      "Answer: The email is an auto-generated newsletter from Axis Bank, informing the recipient about their subscription to the TLDR newsletter. The email provides options for managing subscriptions or unsubscribing from future emails.\n",
      "\n",
      "Based on the context of these emails, I could not find any specific information related to summarizing unread emails. The email solely focuses on providing instructions for managing subscriptions and unsubscribing from the newsletter. Therefore, I cannot provide a summary of unread emails based on this email chain.\n",
      "\n",
      "Source Documents:\n",
      "\n",
      "Source 1:\n",
      "- Subject: Prime =?UTF-8?B?4oK5MjEsNDAwIEFubnVhbCBTYXZpbmdzISDwn5Kw?=\n",
      "- From: IndusInd Bank <indusind_bank@indusind.com>\n",
      "- Date: Wed, 15 Jan 2025 04:58:24 +0000 (UTC)\n",
      "\n",
      "Source 2:\n",
      "- Subject: Your transfer is confirmed\n",
      "- From: \"Xe Money Transfer\" <xe@service.xe.com>\n",
      "- Date: Thu, 20 Feb 2025 04:24:20 +0000\n",
      "\n",
      "Source 3:\n",
      "- Subject: DeepSeek accelerates =?utf-8?Q?=F0=9F=A4=96=2C?= SpaceX moon\n",
      " mission =?utf-8?Q?=F0=9F=9A=80=2C?= Gemini Code Assist now free\n",
      " =?utf-8?Q?=F0=9F=91=A8=E2=80=8D=F0=9F=92=BB?=\n",
      "- From: TLDR <dan@tldrnewsletter.com>\n",
      "- Date: Wed, 26 Feb 2025 11:44:11 +0000\n"
     ]
    }
   ],
   "source": [
    "# thank you Claude.ai\n",
    "\n",
    "import os\n",
    "import mailbox\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Generator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "class LargeGmailMboxRAGAgent:\n",
    "    def __init__(self, mbox_path: str, embedding_model: str = 'nomic-embed-text', persist_directory: str = \"./email_db\"):\n",
    "        \"\"\"\n",
    "        Initialize the Gmail Mbox RAG Agent for large files\n",
    "        \n",
    "        :param mbox_path: Path to the mbox file\n",
    "        :param embedding_model: Embedding model to use\n",
    "        :param persist_directory: Directory to store the vector database\n",
    "        \"\"\"\n",
    "        self.mbox_path = mbox_path\n",
    "        self.persist_directory = persist_directory\n",
    "        self.embeddings = OllamaEmbeddings(model=embedding_model)\n",
    "        self.vectorstore = None\n",
    "        \n",
    "        # Create persistence directory if needed\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "        \n",
    "    def parse_email(self, email_message) -> str:\n",
    "        \"\"\"\n",
    "        Extract text content from an email message\n",
    "        \n",
    "        :param email_message: mailbox email message\n",
    "        :return: Extracted text content\n",
    "        \"\"\"\n",
    "        # Extract metadata\n",
    "        subject = email_message.get('subject', 'No Subject')\n",
    "        if subject is None:\n",
    "            subject = 'No Subject'\n",
    "        \n",
    "        from_email = email_message.get('from', 'Unknown')\n",
    "        date = email_message.get('date', 'Unknown Date')\n",
    "        \n",
    "        # Handle encoding issues with subject\n",
    "        if isinstance(subject, bytes):\n",
    "            try:\n",
    "                subject = subject.decode('utf-8', errors='ignore')\n",
    "            except:\n",
    "                subject = 'Encoding Error in Subject'\n",
    "        \n",
    "        # Extract body\n",
    "        body = ''\n",
    "        \n",
    "        # Handle multipart emails\n",
    "        if email_message.is_multipart():\n",
    "            for part in email_message.walk():\n",
    "                content_type = part.get_content_type()\n",
    "                if content_type == 'text/plain':\n",
    "                    try:\n",
    "                        payload = part.get_payload(decode=True)\n",
    "                        if payload:\n",
    "                            body += payload.decode('utf-8', errors='ignore')\n",
    "                    except Exception as e:\n",
    "                        pass  # Skip problematic parts\n",
    "        else:\n",
    "            try:\n",
    "                payload = email_message.get_payload(decode=True)\n",
    "                if payload:\n",
    "                    body = payload.decode('utf-8', errors='ignore')\n",
    "            except Exception as e:\n",
    "                body = 'Error extracting body'\n",
    "        \n",
    "        # Combine metadata and body\n",
    "        return f\"From: {from_email}\\nDate: {date}\\nSubject: {subject}\\n\\n{body}\"\n",
    "    \n",
    "    def process_emails_in_batches(self, batch_size: int = 500) -> Generator[List[Dict], None, None]:\n",
    "        \"\"\"\n",
    "        Process mbox file in batches to avoid memory issues\n",
    "        \n",
    "        :param batch_size: Number of emails to process in each batch\n",
    "        :return: Generator yielding batches of email documents\n",
    "        \"\"\"\n",
    "        print(f\"Opening mbox file: {self.mbox_path}\")\n",
    "        mbox = mailbox.mbox(self.mbox_path)\n",
    "        total_emails = len(mbox)\n",
    "        print(f\"Found {total_emails} emails in mbox file\")\n",
    "        \n",
    "        batch = []\n",
    "        \n",
    "        for i, message in tqdm(enumerate(mbox), total=total_emails, desc=\"Processing emails\"):\n",
    "            try:\n",
    "                email_text = self.parse_email(message)\n",
    "                batch.append({\n",
    "                    'page_content': email_text,\n",
    "                    'metadata': {\n",
    "                        'source': self.mbox_path,\n",
    "                        'date': message.get('date', 'Unknown Date'),\n",
    "                        'from': message.get('from', 'Unknown'),\n",
    "                        'subject': message.get('subject', 'No Subject')\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                # When batch is full, yield it and clear\n",
    "                if len(batch) >= batch_size:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing email {i}: {e}\")\n",
    "        \n",
    "        # Yield any remaining emails\n",
    "        if batch:\n",
    "            yield batch\n",
    "    \n",
    "    def create_vector_store(self, chunk_size: int = 1000, chunk_overlap: int = 100, batch_size: int = 500):\n",
    "        \"\"\"\n",
    "        Create vector store from processed emails with batching\n",
    "        \n",
    "        :param chunk_size: Size of text chunks\n",
    "        :param chunk_overlap: Overlap between chunks\n",
    "        :param batch_size: Number of emails to process in each batch\n",
    "        \"\"\"\n",
    "        # Initialize text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Check if vectorstore already exists\n",
    "        if os.path.exists(os.path.join(self.persist_directory, 'chroma.sqlite3')):\n",
    "            print(f\"Loading existing vector store from {self.persist_directory}\")\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=self.persist_directory,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # Create new vectorstore\n",
    "        print(\"Creating new vector store\")\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # Process emails in batches\n",
    "        batch_count = 0\n",
    "        total_docs = 0\n",
    "        \n",
    "        for batch in self.process_emails_in_batches(batch_size):\n",
    "            batch_count += 1\n",
    "            print(f\"Processing batch {batch_count} with {len(batch)} emails\")\n",
    "            \n",
    "            # Split documents into chunks\n",
    "            split_docs = text_splitter.create_documents(\n",
    "                [doc['page_content'] for doc in batch],\n",
    "                metadatas=[doc['metadata'] for doc in batch]\n",
    "            )\n",
    "            \n",
    "            total_docs += len(split_docs)\n",
    "            print(f\"Created {len(split_docs)} chunks from this batch\")\n",
    "            \n",
    "            # Add documents to vectorstore\n",
    "            self.vectorstore.add_documents(split_docs)\n",
    "            \n",
    "            # Persist after each batch\n",
    "            self.vectorstore.persist()\n",
    "            \n",
    "            print(f\"Total chunks in vector store: {total_docs}\")\n",
    "    \n",
    "    def create_rag_chain(self, model: str = 'llama2'):\n",
    "        \"\"\"\n",
    "        Create RAG chain for querying emails\n",
    "        \n",
    "        :param model: Ollama LLM model to use\n",
    "        \"\"\"\n",
    "        if not self.vectorstore:\n",
    "            print(\"Vector store not found. Loading or creating...\")\n",
    "            self.create_vector_store()\n",
    "        \n",
    "        # Initialize language model\n",
    "        llm = ChatOllama(model=model, temperature=0.1)\n",
    "        \n",
    "        # Create retriever\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_kwargs={'k': 5}  # Return top 5 most relevant documents\n",
    "        )\n",
    "        \n",
    "        # Custom prompt template\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a helpful AI assistant specialized in analyzing emails.\n",
    "        \n",
    "        Context information from emails:\n",
    "        {context}\n",
    "        \n",
    "        User Question: {question}\n",
    "        \n",
    "        Based on the context of these emails, provide a comprehensive and precise answer.\n",
    "        If the information is not found in the provided emails, say \"I could not find relevant information in the provided emails.\"\n",
    "        Include relevant dates, senders, and subjects when appropriate.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create retrieval QA chain\n",
    "        self.rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "    \n",
    "    def query_emails(self, query: str):\n",
    "        \"\"\"\n",
    "        Query the email corpus\n",
    "        \n",
    "        :param query: User's query\n",
    "        :return: Answer and source documents\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'rag_chain'):\n",
    "            self.create_rag_chain()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = self.rag_chain({\"query\": query, \"question\": query})\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"Query processed in {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        return result['result'], result['source_documents']\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    print(\"Processing...\\n\")\n",
    "    # Initialize the agent\n",
    "    gmail_agent = LargeGmailMboxRAGAgent('/Users/jigneshjain/Documents/top_100_emails.mbox', persist_directory=\"/Users/jigneshjain/Documents/my_large_email_db\")\n",
    "    \n",
    "    # Create vector store\n",
    "    gmail_agent.create_vector_store(batch_size=250)  # Process 250 emails at a time\n",
    "    \n",
    "    # Example query\n",
    "    query = \"Summarize unread emails\"\n",
    "    answer, sources = gmail_agent.query_emails(query)\n",
    "    \n",
    "    print(\"\\nAnswer:\", answer)\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for i, doc in enumerate(sources[:3]):  # Show first 3 sources\n",
    "        print(f\"\\nSource {i+1}:\")\n",
    "        print(f\"- Subject: {doc.metadata.get('subject', 'Unknown')}\")\n",
    "        print(f\"- From: {doc.metadata.get('from', 'Unknown')}\")\n",
    "        print(f\"- Date: {doc.metadata.get('date', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99a8e6a-92b5-4ed2-b598-9cf3d22471cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query processed in 11.18 seconds\n",
      "\n",
      "Answer: Avalon Bay sends emails to residents of Avalon Esterra Park regarding various events and services. The emails are sent from different addresses, including [avalonesterrapark@avalonbay.com](mailto:avalonesterrapark@avalonbay.com) and include the resident's name in the \"To\" field.\n",
      "\n",
      "The emails provide information about upcoming events, such as a Breakfast on the Go session today at 9:30 AM, and refer residents to their team for more details. They also offer insurance benefits and a referral program.\n",
      "\n",
      "The emails are sent from Avalon Bay Communities, Inc., with the email address [avalonbay.com](mailto:avalonbay.com). The emails include the date, subject, and sender's name and email address.\n",
      "\n",
      "Based on the provided emails, I could not find any specific information about the residents' personal details or preferences.\n"
     ]
    }
   ],
   "source": [
    "query = \"Summarize all emails from Avalon bay\"\n",
    "answer, sources = gmail_agent.query_emails(query)\n",
    "\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
